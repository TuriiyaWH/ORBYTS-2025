{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "AY5pPsAmenBW"
      },
      "outputs": [],
      "source": [
        "#rah"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujAxJESreo7e"
      },
      "source": [
        "**^^^ console ^^^**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJNI-FFiuOhb"
      },
      "source": [
        "### **IMPORTS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpFdN-Cw9Nty",
        "outputId": "e412505e-7345-4e68-8c6a-88ff36d71cfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Core Python libraries\n",
        "import os\n",
        "\n",
        "# Scientific computing\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import xarray as xr\n",
        "\n",
        "# PyTorch core\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.nn.init as init\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "\n",
        "# Sklearn utilities\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "# Google Colab (if running in Colab)\n",
        "from google.colab import drive\n",
        "\n",
        "#MOUNT AND R/W DRIVE\n",
        "drive.mount('/content/drive')\n",
        "save_path = \"/content/drive/MyDrive/Weights/best_model.pth\"\n",
        "\n",
        "#GPU or CPU  device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LBMMygpul-Q"
      },
      "source": [
        "### **DOWNLOAD DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuP9Lcrb9Ntz",
        "outputId": "5c69c1de-b590-4f70-e225-541fc10caf39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1gl5W5PexON0wBfajUTPEkd9AiEnHo4rL\n",
            "From (redirected): https://drive.google.com/uc?id=1gl5W5PexON0wBfajUTPEkd9AiEnHo4rL&confirm=t&uuid=7d85f493-cc45-4fa4-a610-fee45f9d7eda\n",
            "To: /content/data.hdf5\n",
            "100% 263M/263M [00:05<00:00, 49.7MB/s]\n"
          ]
        }
      ],
      "source": [
        "if not os.path.exists(\"data.hdf5\"):\n",
        "    !gdown https://drive.google.com/uc?id=1gl5W5PexON0wBfajUTPEkd9AiEnHo4rL -O data.hdf5\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fi34bIcCyboo"
      },
      "source": [
        "### **NN MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_UXD21I9Nt1"
      },
      "outputs": [],
      "source": [
        "class ImprovedModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImprovedModel, self).__init__()\n",
        "\n",
        "        # Increased layer width and depth\n",
        "        self.fc1 = nn.Linear(52, 1024)\n",
        "        self.bn1 = nn.BatchNorm1d(1024)\n",
        "        self.dropout1 = nn.Dropout(0.1)  # Slightly increased\n",
        "\n",
        "        self.fc2 = nn.Linear(1024, 512)\n",
        "        self.bn2 = nn.BatchNorm1d(512)\n",
        "        self.dropout2 = nn.Dropout(0.1)\n",
        "\n",
        "        self.fc3 = nn.Linear(512, 128)\n",
        "        self.bn3 = nn.BatchNorm1d(128)\n",
        "        self.dropout3 = nn.Dropout(0.1)\n",
        "\n",
        "        self.fc4 = nn.Linear(128, 32)  # Extra FC layer\n",
        "        self.fc5 = nn.Linear(32, 1)  # Output layer\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.nn.functional.silu(self.bn1(self.fc1(x)))\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.nn.functional.silu(self.bn2(self.fc2(x)))\n",
        "        x = self.dropout2(x)\n",
        "        x = torch.nn.functional.silu(self.bn3(self.fc3(x)))\n",
        "        x = self.dropout3(x)\n",
        "        x = torch.nn.functional.silu(self.fc4(x))  # Extra layer\n",
        "        x = self.fc5(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for layer in [self.fc1, self.fc2, self.fc3, self.fc4, self.fc5]:\n",
        "            init.kaiming_normal_(layer.weight, nonlinearity='relu', mode='fan_out')\n",
        "            init.constant_(layer.bias, 0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSIqnK9DyrY6"
      },
      "source": [
        "### **INITIALISE VARIABLES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSUIMHd2ywmz"
      },
      "outputs": [],
      "source": [
        "# Initialize\n",
        "model = ImprovedModel().to(device)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "criterion = nn.MSELoss().to(device)\n",
        "lr = 0.00005\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.00005, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100, eta_min=1e-7)\n",
        "batch_size = 1028\n",
        "best_loss = float(\"inf\")\n",
        "patience_counter = 0\n",
        "patience = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaQpznevvTn3"
      },
      "source": [
        "### **LOAD DATA/PREPROCESSING/CREATE DATASET**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0TdcMQpU9Ntz",
        "outputId": "f34858d9-bd75-48c7-f5ac-986706b96abc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset Shapes: X_train (58490, 52), X_val (14623, 52)\n",
            "Target Shapes: y_train (58490,), y_val (14623,)\n",
            "X_train_tensor shape: torch.Size([58490, 52])\n",
            "y_train_tensor shape: torch.Size([58490, 1])\n"
          ]
        }
      ],
      "source": [
        "# 🚀 Load dataset\n",
        "ds = xr.open_dataset(\"data.hdf5\", engine=\"h5netcdf\")\n",
        "\n",
        "# 🚀 Select input features and target variable\n",
        "X = ds[\"spectrum\"].values.astype(np.float32)  # Ensure float32\n",
        "y = ds[\"log_H2O\"].values.astype(np.float32)   # Ensure float32\n",
        "\n",
        "# 🚀 Fix: Check for NaNs and replace or remove them\n",
        "if np.isnan(X).any() or np.isnan(y).any():\n",
        "    print(\"Warning: NaN values detected! Replacing with mean...\")\n",
        "    X = np.nan_to_num(X, nan=np.nanmean(X))\n",
        "    y = np.nan_to_num(y, nan=np.nanmean(y))\n",
        "\n",
        "# 🚀 Fix: Normalize input features\n",
        "scaler_X = StandardScaler()\n",
        "X = scaler_X.fit_transform(X)\n",
        "\n",
        "# 🚀 Fix: Normalize target variable (only if necessary)\n",
        "scaler_y = StandardScaler()\n",
        "y = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()  # Reshape before scaling\n",
        "\n",
        "# 🚀 Fix: Split dataset properly\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 🚀 Fix: Apply the SAME SCALERS to validation data\n",
        "X_val = scaler_X.transform(X_val)  # Apply same transformation\n",
        "y_val = scaler_y.transform(y_val.reshape(-1, 1)).flatten()  # Apply same transformation\n",
        "\n",
        "# 🚀 Convert to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)  # Fix shape\n",
        "\n",
        "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
        "y_val_tensor = torch.tensor(y_val, dtype=torch.float32).view(-1, 1)      # Fix shape\n",
        "\n",
        "# 🚀 Create PyTorch datasets\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "\n",
        "# 🚀 Load data into DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# 🚀 Print some debug info\n",
        "print(f\"Dataset Shapes: X_train {X_train.shape}, X_val {X_val.shape}\")\n",
        "print(f\"Target Shapes: y_train {y_train.shape}, y_val {y_val.shape}\")\n",
        "\n",
        "print(f\"X_train_tensor shape: {X_train_tensor.shape}\")\n",
        "print(f\"y_train_tensor shape: {y_train_tensor.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrLJt09DzAgD"
      },
      "source": [
        "### **CLASSES AND FUNCTIONS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q87iFvZAquPC"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=100, min_delta=0.001):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.best_loss = np.inf\n",
        "        self.counter = 0\n",
        "\n",
        "    def check_early_stop(self, current_loss):\n",
        "        if current_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = current_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                print(f\"Stopping early at epoch {epoch}!\")\n",
        "                return True\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDRFiERR9Nt4"
      },
      "outputs": [],
      "source": [
        "def plot_loss(losses, epoch):\n",
        "\n",
        "    losses = np.array(losses)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(losses, \"k-\")\n",
        "\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('loss')\n",
        "    plt.title(f'training loss\\nepoch: {epoch}')\n",
        "\n",
        "    plt.yscale('log')\n",
        "\n",
        "    plt.ylim(losses.mean() - 3 * losses.std(), 4)\n",
        "\n",
        "    plt.savefig('training_loss.png')\n",
        "    plt.close()\n",
        "\n",
        "def plot_valloss(losses, epoch):\n",
        "\n",
        "    losses = np.array(losses)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(losses, \"k-\")\n",
        "\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('val_loss')\n",
        "    plt.title(f'Validation loss\\nepoch: {epoch}')\n",
        "\n",
        "    plt.yscale('log')\n",
        "\n",
        "    plt.ylim(losses.mean() - 3 * losses.std(), 4)\n",
        "\n",
        "    plt.savefig('validation_loss.png')\n",
        "    plt.close()\n",
        "\n",
        "def plot_predictions(predictions, labels, epoch):\n",
        "    # make sure plot is square\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    plt.plot(labels, predictions, \"k.\", label='predictions')\n",
        "    plt.plot(labels, labels, \"r--\", label='Ground Truth')\n",
        "    plt.xlabel('true log H2O value')\n",
        "    plt.ylabel('predicted H2O value')\n",
        "    plt.title(f'predictions\\nepoch: {epoch}')\n",
        "\n",
        "    plt.xlim(labels.min(), labels.max())\n",
        "    plt.ylim(labels.min(), labels.max())\n",
        "\n",
        "    plt.savefig('predictions.png')\n",
        "    plt.close()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XTlIXYnq3Pv"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(epoch, model, optimizer, loss, save_path=save_path):\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss\n",
        "    }, save_path)\n",
        "    print(f\"🔥 Saved New Best Model at Epoch {epoch} with Loss: {loss}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvaWuV9RrGR-"
      },
      "outputs": [],
      "source": [
        "def load_checkpoint(model, optimizer, path=save_path):\n",
        "    if not os.path.exists(path):\n",
        "        print(\"❌ No checkpoint found. Starting from scratch.\")\n",
        "        return 0, float('inf')\n",
        "    try:\n",
        "        checkpoint = torch.load(path, map_location=torch.device('cpu'), weights_only = False)  # Add map_location if using Colab\n",
        "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "        print(f\"✅ Loaded Best Model from Epoch {checkpoint['epoch']} with Loss: {checkpoint['loss']}\")\n",
        "        return checkpoint[\"epoch\"], checkpoint[\"loss\"]\n",
        "    except RuntimeError as e:\n",
        "        print(\"⚠️ Checkpoint loading failed due to model mismatch or architecture change. Starting from scratch.\")\n",
        "        print(e)\n",
        "        return 0, float('inf')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCnWmPkl1i-n"
      },
      "source": [
        "### **TRAINING SCRIPT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8lmETug9Nt5",
        "outputId": "b8068d8b-cd86-4dd7-fb3b-37b3f9ead14c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Loaded Best Model from Epoch 504 with Loss: 0.5038498221782216\n",
            "🔥 Saved New Best Model at Epoch 504 with Loss: 0.5033683457918334\n",
            "🔥 Saved New Best Model at Epoch 504 with Val Loss: 10.5023\n",
            "Epoch 504, Train Loss: 0.5034, Val Loss: 10.5023\n",
            "Epoch 505, Train Loss: 0.5032, Val Loss: 17.3228\n",
            "🔥 Saved New Best Model at Epoch 506 with Loss: 0.5042569652984017\n",
            "🔥 Saved New Best Model at Epoch 506 with Val Loss: 7.9216\n",
            "Epoch 506, Train Loss: 0.5043, Val Loss: 7.9216\n",
            "Epoch 507, Train Loss: 0.5027, Val Loss: 9.3790\n",
            "Epoch 508, Train Loss: 0.4942, Val Loss: 10.5603\n",
            "Epoch 509, Train Loss: 0.4978, Val Loss: 17.9231\n",
            "Epoch 510, Train Loss: 0.4966, Val Loss: 9.1646\n",
            "🔥 Saved New Best Model at Epoch 511 with Loss: 0.5082720988675168\n",
            "🔥 Saved New Best Model at Epoch 511 with Val Loss: 7.3137\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-7-d223f4f6e398>:32: UserWarning: Attempt to set non-positive ylim on a log-scaled axis will be ignored.\n",
            "  plt.ylim(losses.mean() - 3 * losses.std(), 4)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 511, Train Loss: 0.5083, Val Loss: 7.3137\n",
            "Epoch 512, Train Loss: 0.5009, Val Loss: 7.9116\n",
            "Epoch 513, Train Loss: 0.5065, Val Loss: 17.6291\n",
            "🔥 Saved New Best Model at Epoch 514 with Loss: 0.4948714291840269\n",
            "🔥 Saved New Best Model at Epoch 514 with Val Loss: 6.9559\n",
            "Epoch 514, Train Loss: 0.4949, Val Loss: 6.9559\n",
            "Epoch 515, Train Loss: 0.5039, Val Loss: 7.3294\n",
            "Epoch 516, Train Loss: 0.5056, Val Loss: 11.2197\n",
            "Epoch 517, Train Loss: 0.4942, Val Loss: 11.3329\n",
            "Epoch 518, Train Loss: 0.4991, Val Loss: 8.2085\n",
            "Epoch 519, Train Loss: 0.5063, Val Loss: 12.0798\n",
            "Epoch 520, Train Loss: 0.5010, Val Loss: 12.1868\n",
            "🔥 Saved New Best Model at Epoch 521 with Loss: 0.4983705496578886\n",
            "🔥 Saved New Best Model at Epoch 521 with Val Loss: 6.9186\n",
            "Epoch 521, Train Loss: 0.4984, Val Loss: 6.9186\n",
            "Epoch 522, Train Loss: 0.4997, Val Loss: 9.1570\n",
            "Epoch 523, Train Loss: 0.5022, Val Loss: 6.9465\n",
            "🔥 Saved New Best Model at Epoch 524 with Loss: 0.5023060822696016\n",
            "🔥 Saved New Best Model at Epoch 524 with Val Loss: 6.8668\n",
            "Epoch 524, Train Loss: 0.5023, Val Loss: 6.8668\n",
            "Epoch 525, Train Loss: 0.5000, Val Loss: 8.6570\n",
            "Epoch 526, Train Loss: 0.5049, Val Loss: 9.9512\n",
            "Epoch 527, Train Loss: 0.5054, Val Loss: 17.1975\n",
            "Epoch 528, Train Loss: 0.4996, Val Loss: 7.9697\n",
            "Epoch 529, Train Loss: 0.5005, Val Loss: 7.6260\n",
            "Epoch 530, Train Loss: 0.4979, Val Loss: 7.2371\n",
            "Epoch 531, Train Loss: 0.5058, Val Loss: 7.0798\n",
            "🔥 Saved New Best Model at Epoch 532 with Loss: 0.49730229482316135\n",
            "🔥 Saved New Best Model at Epoch 532 with Val Loss: 6.0372\n",
            "Epoch 532, Train Loss: 0.4973, Val Loss: 6.0372\n",
            "Epoch 533, Train Loss: 0.5077, Val Loss: 7.3317\n",
            "Epoch 534, Train Loss: 0.5046, Val Loss: 8.7813\n",
            "Epoch 535, Train Loss: 0.5003, Val Loss: 10.9092\n",
            "Epoch 536, Train Loss: 0.5019, Val Loss: 7.1707\n",
            "Epoch 537, Train Loss: 0.5057, Val Loss: 6.9174\n",
            "Epoch 538, Train Loss: 0.5042, Val Loss: 6.4075\n",
            "Epoch 539, Train Loss: 0.5017, Val Loss: 8.7837\n",
            "Epoch 540, Train Loss: 0.5011, Val Loss: 8.8058\n",
            "Epoch 541, Train Loss: 0.4984, Val Loss: 16.2227\n",
            "Epoch 542, Train Loss: 0.4966, Val Loss: 7.4744\n",
            "Epoch 543, Train Loss: 0.5048, Val Loss: 45.8419\n",
            "Epoch 544, Train Loss: 0.5002, Val Loss: 8.5611\n",
            "Epoch 545, Train Loss: 0.5032, Val Loss: 10.0011\n",
            "Epoch 546, Train Loss: 0.4991, Val Loss: 10.6997\n",
            "Epoch 547, Train Loss: 0.5034, Val Loss: 8.9680\n",
            "Epoch 548, Train Loss: 0.5054, Val Loss: 10.4378\n",
            "Epoch 549, Train Loss: 0.5021, Val Loss: 7.2929\n",
            "Epoch 550, Train Loss: 0.5067, Val Loss: 9.1067\n",
            "Epoch 551, Train Loss: 0.5105, Val Loss: 9.5798\n",
            "Epoch 552, Train Loss: 0.5046, Val Loss: 8.4127\n",
            "Epoch 553, Train Loss: 0.5076, Val Loss: 10.0163\n",
            "Epoch 554, Train Loss: 0.5034, Val Loss: 7.1940\n",
            "Epoch 555, Train Loss: 0.4928, Val Loss: 8.3781\n",
            "Epoch 556, Train Loss: 0.4968, Val Loss: 7.4964\n",
            "Epoch 557, Train Loss: 0.5041, Val Loss: 26.3554\n",
            "Epoch 558, Train Loss: 0.5022, Val Loss: 8.2278\n",
            "Epoch 559, Train Loss: 0.5112, Val Loss: 6.7344\n",
            "Epoch 560, Train Loss: 0.5029, Val Loss: 12.9727\n",
            "Epoch 561, Train Loss: 0.5023, Val Loss: 8.6871\n",
            "Epoch 562, Train Loss: 0.5039, Val Loss: 6.9276\n",
            "Epoch 563, Train Loss: 0.5005, Val Loss: 9.4474\n",
            "Epoch 564, Train Loss: 0.5102, Val Loss: 8.8164\n",
            "Epoch 565, Train Loss: 0.5002, Val Loss: 8.5812\n",
            "Epoch 566, Train Loss: 0.4991, Val Loss: 7.1259\n",
            "Epoch 567, Train Loss: 0.5031, Val Loss: 9.8064\n",
            "Epoch 568, Train Loss: 0.5044, Val Loss: 7.8636\n",
            "Epoch 569, Train Loss: 0.5031, Val Loss: 8.7837\n",
            "Epoch 570, Train Loss: 0.5111, Val Loss: 12.1903\n",
            "🔥 Saved New Best Model at Epoch 571 with Loss: 0.501735587391937\n",
            "🔥 Saved New Best Model at Epoch 571 with Val Loss: 5.9106\n",
            "Epoch 571, Train Loss: 0.5017, Val Loss: 5.9106\n",
            "Epoch 572, Train Loss: 0.5167, Val Loss: 24.4344\n",
            "Epoch 573, Train Loss: 0.5034, Val Loss: 7.3972\n",
            "Epoch 574, Train Loss: 0.5035, Val Loss: 7.5527\n",
            "Epoch 575, Train Loss: 0.4959, Val Loss: 7.4784\n",
            "Epoch 576, Train Loss: 0.4996, Val Loss: 7.4064\n",
            "Epoch 577, Train Loss: 0.5096, Val Loss: 9.8991\n",
            "Epoch 578, Train Loss: 0.5069, Val Loss: 7.7895\n",
            "Epoch 579, Train Loss: 0.5035, Val Loss: 11.2470\n",
            "Epoch 580, Train Loss: 0.5029, Val Loss: 22.8618\n",
            "Epoch 581, Train Loss: 0.5040, Val Loss: 9.7454\n",
            "Epoch 582, Train Loss: 0.4967, Val Loss: 7.8216\n",
            "Epoch 583, Train Loss: 0.5007, Val Loss: 7.7944\n",
            "Epoch 584, Train Loss: 0.5017, Val Loss: 7.8383\n",
            "Epoch 585, Train Loss: 0.4992, Val Loss: 6.7944\n",
            "Epoch 586, Train Loss: 0.5003, Val Loss: 7.1940\n",
            "Epoch 587, Train Loss: 0.5046, Val Loss: 9.3266\n",
            "Epoch 588, Train Loss: 0.4984, Val Loss: 8.8692\n",
            "Epoch 589, Train Loss: 0.5061, Val Loss: 16.2141\n",
            "Epoch 590, Train Loss: 0.5010, Val Loss: 10.7539\n",
            "Epoch 591, Train Loss: 0.5010, Val Loss: 7.3843\n",
            "Epoch 592, Train Loss: 0.4987, Val Loss: 9.1252\n",
            "Epoch 593, Train Loss: 0.4990, Val Loss: 7.3415\n",
            "Epoch 594, Train Loss: 0.5015, Val Loss: 9.4424\n",
            "Epoch 595, Train Loss: 0.5021, Val Loss: 6.4030\n",
            "Epoch 596, Train Loss: 0.5078, Val Loss: 42.6396\n",
            "Epoch 597, Train Loss: 0.5010, Val Loss: 7.5884\n",
            "Epoch 598, Train Loss: 0.5004, Val Loss: 8.1201\n",
            "Epoch 599, Train Loss: 0.5093, Val Loss: 7.8862\n",
            "Epoch 600, Train Loss: 0.4998, Val Loss: 8.9590\n",
            "Epoch 601, Train Loss: 0.4979, Val Loss: 7.9183\n",
            "Epoch 602, Train Loss: 0.5056, Val Loss: 10.7529\n",
            "Epoch 603, Train Loss: 0.5040, Val Loss: 9.5593\n",
            "Epoch 604, Train Loss: 0.5078, Val Loss: 7.8039\n",
            "Epoch 605, Train Loss: 0.5015, Val Loss: 6.7866\n",
            "Epoch 606, Train Loss: 0.5036, Val Loss: 8.2014\n",
            "Epoch 607, Train Loss: 0.5010, Val Loss: 10.0845\n",
            "Epoch 608, Train Loss: 0.5092, Val Loss: 9.0138\n",
            "Epoch 609, Train Loss: 0.5004, Val Loss: 8.6556\n",
            "Epoch 610, Train Loss: 0.5033, Val Loss: 7.9736\n",
            "Epoch 611, Train Loss: 0.4979, Val Loss: 7.7060\n",
            "Epoch 612, Train Loss: 0.4986, Val Loss: 7.3686\n",
            "Epoch 613, Train Loss: 0.4987, Val Loss: 7.0150\n",
            "Epoch 614, Train Loss: 0.4957, Val Loss: 8.6213\n",
            "Epoch 615, Train Loss: 0.5065, Val Loss: 28.4534\n",
            "Epoch 616, Train Loss: 0.5041, Val Loss: 6.8116\n",
            "Epoch 617, Train Loss: 0.4992, Val Loss: 7.0952\n",
            "Epoch 618, Train Loss: 0.4948, Val Loss: 7.6031\n",
            "Epoch 619, Train Loss: 0.4989, Val Loss: 10.1316\n",
            "Epoch 620, Train Loss: 0.5055, Val Loss: 6.7722\n",
            "Epoch 621, Train Loss: 0.4911, Val Loss: 15.3377\n",
            "Stopping early at epoch 621!\n",
            "⏹ Stopping Early at Epoch 621 (No Improvement for 50 epochs), lr=5e-05\n"
          ]
        }
      ],
      "source": [
        "train_losses = []\n",
        "val_losses = []\n",
        "batch_losses = []\n",
        "epoch_losses = []\n",
        "patience_counter = 0\n",
        "patience = 50\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "try:\n",
        "    start_epoch, best_loss = load_checkpoint(model, optimizer)\n",
        "except RuntimeError as e:\n",
        "    print(\"⚠️ Checkpoint loading failed due to model mismatch. Starting from scratch.\")\n",
        "    print(e)\n",
        "    start_epoch = 0\n",
        "    best_loss = float('inf')\n",
        "\n",
        "early_stopping = EarlyStopping(patience=patience)\n",
        "\n",
        "# 🚀 TRAINING LOOP\n",
        "best_val_loss = float(\"inf\")  # Track best validation loss\n",
        "\n",
        "for epoch in range(start_epoch, 1000):\n",
        "    model.train()\n",
        "    batch_losses = []\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_losses.append(loss.item())\n",
        "\n",
        "    epoch_loss = np.mean(batch_losses)\n",
        "    epoch_losses.append(epoch_loss)\n",
        "\n",
        "    # 🔍 VALIDATION LOOP\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "    val_loss /= len(val_loader)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    # 💾 Save the best model based on VALIDATION loss\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        save_checkpoint(epoch, model, optimizer, epoch_loss)\n",
        "        print(f\"🔥 Saved New Best Model at Epoch {epoch} with Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # Adjust learning rate\n",
        "    scheduler.step()\n",
        "    plot_loss(epoch_losses, epoch)\n",
        "    plot_valloss(val_losses,epoch)\n",
        "    plot_predictions(outputs.detach().cpu().numpy(), labels.detach().cpu().numpy(), epoch)\n",
        "\n",
        "\n",
        "    # 📊 Logging\n",
        "    print(f\"Epoch {epoch}, Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # ⏹ Early stopping\n",
        "    if early_stopping.check_early_stop(val_loss):\n",
        "        print(f\"⏹ Stopping Early at Epoch {epoch} (No Improvement for {patience} epochs), lr={lr}\")\n",
        "        break\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}